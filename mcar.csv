"import gym
import numpy as np

# Initialize the Mountain Car environment
env = gym.make('MountainCar-v0')

# Set hyperparameters
alpha = 0.1  
gamma = 0.99  
epsilon = 1.0  
epsilon_min = 0.01  
epsilon_decay = 0.995  
episodes = 1000  
max_steps = 200  

# Initialize Q-table
state_space_size = [20, 20]
action_space_size = env.action_space.n
q_table = np.zeros((state_space_size[0], state_space_size[1], action_space_size))

# Define a function to discretize the continuous state space
def discretize_state(state):
    state_min = env.observation_space.low
    state_max = env.observation_space.high
    state_bins = (state_max - state_min) / state_space_size
    state = (state - state_min) / state_bins
    return tuple(state.astype(int))

# Loop through episodes
for episode in range(episodes):
    state = env.reset()
    state = discretize_state(state)
    done = False
    total_reward = 0
    
    for step in range(max_steps):
        # Choose an action
        if np.random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  
        else:
            action = np.argmax(q_table[state])
        
        # Take the action and observe the next state and reward
        next_state, reward, done, _ = env.step(action)
        next_state = discretize_state(next_state)
        
        # Update the Q-table
        q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * np.max(q_table[next_state]))
        
        state = next_state
        total_reward += reward
        
        if done:
            break
    
    # Decay the exploration rate
    epsilon = max(epsilon_min, epsilon_decay * epsilon)
    
    # Print the total reward for the episode
    print(""Episode: {}, Total Reward: {}"".format(episode, total_reward))"
