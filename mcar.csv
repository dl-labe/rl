"import gym
import numpy as np

# Create the MountainCar environment
env = gym.make('MountainCar-v0')

# Set hyperparameters
num_episodes = 1000
max_steps = 200
learning_rate = 0.1
discount_factor = 0.99
exploration_rate = 1.0
min_exploration_rate = 0.01
exploration_decay_rate = 0.01

# Initialize Q-table
num_states = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])
num_states = np.round(num_states, 0).astype(int) + 1
q_table = np.zeros((num_states[0], num_states[1], env.action_space.n))

# Define the discretization function for state space
def discretize_state(state):
    discretized_state = (state - env.observation_space.low) * np.array([10, 100])
    discretized_state = np.round(discretized_state, 0).astype(int)
    return discretized_state

# Q-learning algorithm
total_rewards = []
for episode in range(num_episodes):
    state = env.reset()
    discretized_state = discretize_state(state)
    total_reward = 0
    done = False

    for step in range(max_steps):
        # Exploration-exploitation trade-off
        exploration_rate_threshold = np.random.uniform(0, 1)
        if exploration_rate_threshold > exploration_rate:
            action = np.argmax(q_table[discretized_state[0], discretized_state[1]])
        else:
            action = env.action_space.sample()

        # Take action and observe new state and reward
        new_state, reward, done, _ = env.step(action)
        new_discretized_state = discretize_state(new_state)

        # Update Q-table
        q_table[discretized_state[0], discretized_state[1], action] = \
            (1 - learning_rate) * q_table[discretized_state[0], discretized_state[1], action] + \
            learning_rate * (reward + discount_factor * np.max(q_table[new_discretized_state[0], new_discretized_state[1]]))

        discretized_state = new_discretized_state
        total_reward += reward

        if done:
            break

    # Decay exploration rate
    exploration_rate = min_exploration_rate + \
        (1 - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)

    total_rewards.append(total_reward)

# Calculate average reward
average_reward = sum(total_rewards) / num_episodes
print(f""Average reward over {num_episodes} episodes: {average_reward}"")"
